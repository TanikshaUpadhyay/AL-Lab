{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "LkeGDXu5G3Y-",
      "metadata": {
        "id": "LkeGDXu5G3Y-"
      },
      "source": [
        "## Assignment 4\n",
        "\n",
        "- **Name:** Taniksha Upadhyay \n",
        "- **PRN:** 22070521151\n",
        "- **Subject:** AI Lab\n",
        "- **Section:** B\n",
        "- **Batch:** 2022-26 \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd103778",
      "metadata": {},
      "source": [
        "### 3. Example Text Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "26a5620e",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "text_data = \"Machine learning enables computers to learn patterns and make decisions without explicit programming.\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6019055d",
      "metadata": {},
      "source": [
        "### Import Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "3804acf0",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "022210c9",
      "metadata": {},
      "source": [
        "### Download Necessary NLTK Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ac0b06c5",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "527a5fb9",
      "metadata": {},
      "source": [
        "### Perform Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "6afacf9e",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "tokenized_words = word_tokenize(text_data)\n",
        "tokenized_sentences = sent_tokenize(text_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "725b324a",
      "metadata": {},
      "source": [
        "### Stopwords Removal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "15b1e6ae",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "stopword_list = set(stopwords.words('english'))\n",
        "filtered_tokens = [token for token in tokenized_words if token.lower() not in stopword_list]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f439a7e",
      "metadata": {},
      "source": [
        "### Apply Stemming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "fce8bd83",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ce95703",
      "metadata": {},
      "source": [
        "### Apply Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "dd7692a9",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "536f9c11",
      "metadata": {},
      "source": [
        "### Display Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "eac9bcb9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Text: Machine learning enables computers to learn patterns and make decisions without explicit programming.\n",
            "\n",
            "Tokenized Words: ['Machine', 'learning', 'enables', 'computers', 'to', 'learn', 'patterns', 'and', 'make', 'decisions', 'without', 'explicit', 'programming', '.']\n",
            "\n",
            "Tokenized Sentences: ['Machine learning enables computers to learn patterns and make decisions without explicit programming.']\n",
            "\n",
            "Filtered Words (Stopword Removal): ['Machine', 'learning', 'enables', 'computers', 'learn', 'patterns', 'make', 'decisions', 'without', 'explicit', 'programming', '.']\n",
            "\n",
            "Stemmed Words: ['machin', 'learn', 'enabl', 'comput', 'learn', 'pattern', 'make', 'decis', 'without', 'explicit', 'program', '.']\n",
            "\n",
            "Lemmatized Words: ['Machine', 'learning', 'enables', 'computer', 'learn', 'pattern', 'make', 'decision', 'without', 'explicit', 'programming', '.']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"Original Text:\", text_data)\n",
        "print(\"\\nTokenized Words:\", tokenized_words)\n",
        "print(\"\\nTokenized Sentences:\", tokenized_sentences)\n",
        "print(\"\\nFiltered Words (Stopword Removal):\", filtered_tokens)\n",
        "print(\"\\nStemmed Words:\", stemmed_tokens)\n",
        "print(\"\\nLemmatized Words:\", lemmatized_tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5af504c8",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
